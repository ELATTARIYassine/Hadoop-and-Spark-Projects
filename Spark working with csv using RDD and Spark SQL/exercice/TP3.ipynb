{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP3 \n",
    "#### EL ATTARI Yassine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1\n",
    "   \n",
    "   Créez un programme Spark pour lire les données \"houses\"  dans de fichier RealEstate.csv,\n",
    "   regrouper par emplacement, agréger le prix moyen par SQ Ft et trier par prix moyen par SQ Ft.\n",
    "\n",
    "    L'ensemble de données contient les champs suivants:\n",
    "    1. MLS: Numéro de service d'inscription multiple pour la maison (ID unique).\n",
    "    2. Location: ville / village où se trouve la maison. La plupart des emplacements sont\n",
    "        Comté de San Luis Obispo et nord du comté de Santa Barbara (Santa MariaOrcutt, Lompoc,\n",
    "        Guadelupe, Los Alamos), mais il y a aussi des endroits hors de la région.\n",
    "    3. Price: le prix d'inscription le plus récent de la maison (en dollars).\n",
    "    4. Bedrooms: nombre de chambres.\n",
    "    5. Bathrooms: nombre de salles de bain.\n",
    "    6. Size: taille de la maison en pieds carrés.\n",
    "    7. Price / SQ.ft: prix de la maison au pied carré.\n",
    "    8. Statut: type de vente. Les types sont représentés dans le jeu de données:  Short Sale, \n",
    "        Foreclosure and Regular.\n",
    "\n",
    "    Chaque champ est séparé par des virgules.\n",
    "\n",
    "    Exemple de sortie:\n",
    "\n",
    "    + ---------------- + ----------------- +\n",
    "    | Localisation     | avg (Prix SQ Ft) |\n",
    "    + ---------------- + ----------------- +\n",
    "    | Oceano           | 95,0 |\n",
    "    | Bradley          | 206,0 |\n",
    "    | San Luis Obispo  | 359,0 |\n",
    "    | Santa Ynez       | 491,4 |\n",
    "    | Cayucos          | 887,0 |\n",
    "    | ................ | ................. |\n",
    "    | ................ | ................. |\n",
    "    | ................ | ................. |\n",
    "    '' '\n",
    "    en utilisant RDD et Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EX1 : En utilisant RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('New Cuyama', 34.05), ('Bakersfield', 69.69), ('King City', 71.51333333333334), ('Greenfield', 91.58), ('Santa Margarita', 95.38), ('Soledad', 102.69333333333333), ('Out Of Area', 116.23333333333333), ('Guadalupe', 120.175), ('Coalinga', 124.34285714285714), ('Santa Maria-Orcutt', 147.58871698113194)]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import csv\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    spark = pyspark.sql.SparkSession.builder.master(\"local[*]\").appName(\"BD Real Estate\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO\n",
    "\n",
    "inputfile=\"file://\"+ os.getcwd() +\"/RealEstate.csv\"\n",
    "outputdir=\"file://\"+ os.getcwd() +\"/output/\"\n",
    "\n",
    "## en utilisant RDD\n",
    "\n",
    "def loadRecord(line,header,delimiter):\n",
    "    \"\"\"Parse a CSV line\"\"\"\n",
    "    input = StringIO(line)\n",
    "    reader = csv.DictReader(input, delimiter=delimiter, fieldnames=header)\n",
    "    return next(reader)\n",
    "delimiter = \",\"\n",
    "\n",
    "input = sc.textFile(inputfile)\n",
    "header = input.first().split(delimiter)\n",
    "data = input.filter(lambda x: header[0] not in x).map(lambda x:loadRecord(x,header,delimiter))\n",
    "\n",
    "locationAndPrice = data.map(lambda ligne: (ligne['Location'],ligne['Price SQ Ft']))\n",
    "\n",
    "preparingData = locationAndPrice.groupByKey().map(lambda x : (x[0], sum([float(i) for i in list(x[1])]), len(list(x[1])), sum([float(i) for i in list(x[1])])/ len(list(x[1]))))\n",
    "\n",
    "result = preparingData.map(lambda el:(el[0], el[3])).sortBy(lambda a: a[1])\n",
    "\n",
    "rs = result.take(10)\n",
    "\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EX1 : En utilisant Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|       Location|         avgPrice|\n",
      "+---------------+-----------------+\n",
      "|     New Cuyama|            34.05|\n",
      "|    Bakersfield|            69.69|\n",
      "|      King City|71.51333333333334|\n",
      "|     Greenfield|            91.58|\n",
      "|Santa Margarita|            95.38|\n",
      "+---------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "input_df = spark.read.options(header='true',inferschema='true',delimiter=delimiter).csv(\"file://\"+os.getcwd()+\"/RealEstate.csv\")\n",
    "#poo = input_df.select('Location').avg('Price SQ Ft').show(2)\n",
    "poo = (input_df\n",
    ".groupBy(\"Location\")\n",
    ".avg(\"Price SQ Ft\")\n",
    ".withColumnRenamed(\"avg(Price SQ Ft)\", \"avgPrice\")\n",
    ".orderBy(asc(\"avgPrice\")))\n",
    "print(poo.show(5))\n",
    "#print(input_df.show(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier \"nasa_19950701.tsv\" contient 10000 lignes de journal provenant de l'un des serveurs apache de la NASA pour le 1er juillet 1995.\n",
    "     Le fichier \"nasa_19950801.tsv\" contient 10 000 lignes de journal pour le 1er août 1995\n",
    "     Créez un programme Spark pour générer un nouveau RDD(Dataframe) contenant les hôtes auxquels on accède les DEUX jours.\n",
    "     Enregistrez le RDD (Dataframe) résultant dans le fichier \"out/nasa_logs_same_hosts.csv\".\n",
    "\n",
    "     Exemple de sortie:\n",
    "     vagrant.vf.mmc.com\n",
    "     www-a1.proxy.aol.com\n",
    "     .....\n",
    "\n",
    " Gardez à l'esprit que les fichiers journaux d'origine contiennent les lignes d'en-tête suivantes.\n",
    "    host    logname    time    method    url    response    bytes\n",
    " Assurez-vous que les lignes d'en-tête sont supprimées dans le RDD résultant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EX2 : En utilisant RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import csv\n",
    "try:\n",
    "    sc\n",
    "except NameError:\n",
    "    spark = pyspark.sql.SparkSession.builder.master(\"local[*]\").appName(\"BD Nasa\").getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO\n",
    "\n",
    "inputFirstfile=\"file://\"+ os.getcwd() +\"/nasa_19950701.tsv\"\n",
    "inputSecondfile=\"file://\"+ os.getcwd() +\"/nasa_19950801.tsv\"\n",
    "outputdir=\"file://\"+ os.getcwd() +\"/out/nasa_logs_same_hosts.csv\"\n",
    "\n",
    "## en utilisant RDD\n",
    "\n",
    "def loadRecord(line,header,delimiter):\n",
    "    \"\"\"Parse a CSV line\"\"\"\n",
    "    input = StringIO(line)\n",
    "    reader = csv.DictReader(input, delimiter=delimiter, fieldnames=header)\n",
    "    return next(reader)\n",
    "delimiter = \",\"\n",
    "\n",
    "# 1ère fichier\n",
    "input = sc.textFile(inputFirstfile)\n",
    "header = input.first().split(delimiter)\n",
    "data = input.filter(lambda x: header[0] not in x).map(lambda x:loadRecord(x,header,delimiter))\n",
    "\n",
    "# 2ème fichier\n",
    "input = sc.textFile(inputSecondfile)\n",
    "header = input.first().split(delimiter)\n",
    "secondData = input.filter(lambda x: header[0] not in x).map(lambda x:loadRecord(x,header,delimiter))\n",
    "\n",
    "# traitement pour récupérer une liste de host pour chaque fichier\n",
    "firstFile = data.map(lambda ligne: ligne['host\\tlogname\\ttime\\tmethod\\turl\\tresponse\\tbytes'].split('\\t')[0])\n",
    "secondFile = secondData.map(lambda ligne: ligne['host\\tlogname\\ttime\\tmethod\\turl\\tresponse\\tbytes'].split('\\t')[0])\n",
    "\n",
    "result = firstFile.intersection(secondFile) \n",
    "\n",
    "# préparation des données à sauvegarder \n",
    "rows_of_csv=result.map(lambda content:content.split(',')[0])\n",
    "# enregistrer des données\n",
    "rows_of_csv.saveAsTextFile(outputdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultat \n",
    "\n",
    "##### contenu du fichier part-00000\n",
    "\n",
    "- alyssa.prodigy.com\n",
    "- www-a1.proxy.aol.com\n",
    "- www-d3.proxy.aol.com\n",
    "- piweba1y.prodigy.com\n",
    "- www-b2.proxy.aol.com\n",
    "- www-b3.proxy.aol.com\n",
    "- www-d4.proxy.aol.com\n",
    "- koala.melbpc.org.au\n",
    "- ***suite de fichier***\n",
    "\n",
    "##### contenu du fichier part-00001\n",
    "\n",
    "- piweba3y.prodigy.com\n",
    "- ottgate2.bnr.ca\n",
    "- wwwproxy.info.au\n",
    "- magicall.dacom.co.kr\n",
    "- palona1.cns.hp.com\n",
    "- www-b5.proxy.aol.com\n",
    "- ***suite de fichier***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EX2 : En utilisant Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                host|\n",
      "+--------------------+\n",
      "|        hella.stm.it|\n",
      "|srv1.freenet.calg...|\n",
      "|      ntigate.nt.com|\n",
      "|www-b5.proxy.aol.com|\n",
      "|piweba2y.prodigy.com|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# afin d'obtenir un dataframe bien structuré je devais d'utiliser sep au lieu de delimiter pour le fichier tsv\n",
    "first_df = spark.read.options(header='true',inferschema='true',sep=r'\\t').csv(\"file://\"+os.getcwd()+\"/nasa_19950701.tsv\").select('host').distinct()\n",
    "second_df = spark.read.options(header='true',inferschema='true',sep=r'\\t').csv(\"file://\"+os.getcwd()+\"/nasa_19950801.tsv\").select('host').distinct()\n",
    "\n",
    "# Jointure\n",
    "res = first_df.join(second_df, on='host', how='inner')\n",
    "\n",
    "# chemin du csv\n",
    "secondOutputdir=\"file://\"+ os.getcwd() +\"/out/nasa_logs_same_hosts_second.csv\"\n",
    "\n",
    "# par défaut il génère de nombreuses partitions c'est mieux de limiter la résultat à  1 seul fichier\n",
    "res.repartition(1).write.csv(secondOutputdir)\n",
    "print(distinctDF.show(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### La même résultat 37 ligne en totale "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
