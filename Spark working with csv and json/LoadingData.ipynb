{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement de données dans RDD, DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, Spark ne remplacera pas le dossier de sortie. Pour `RDDs`, on peut passer un paramètre de configuration pour modifier que` .config (\"spark.hadoop.validateOutputSpecs\", \"false\") `pour` DataFrame`s nous définirons explicitement le mode d'écriture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, initialisez la session Spark et le contexte Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "try:\n",
    "    sc\n",
    "except NameError:    \n",
    "    spark = pyspark.sql.SparkSession.builder.master(\"local[*]\").appName(\"hadoop course\").config(\"spark.hadoop.validateOutputSpecs\", \"false\").getOrCreate()\n",
    "    sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger des données non structurées\n",
    "\n",
    "L'une des premières choses que nous devons apprendre est de savoir comment lire les données dans les RDD et les dataframes Spark. Spark fournit une API  pour lire des données structurées dans la plupart des formats de données (CSV, JSON, ...etc) ainsi que des données non structurées (fichiers texte brut, journaux de serveur, etc.).\n",
    "\n",
    "### Lecture de texte brut\n",
    "\n",
    "`textFile` et` wholeTextFiles` sont des fonctions à lire le texte brut non structuré.\n",
    "\n",
    "1. `textFile` lit les données ligne par ligne créant un RDD où chaque entrée correspond à une ligne (un peu comme readlines () en Python)\n",
    "1. `wholeTextFiles` lit le fichier entier dans une paire RDD: (chemin du fichier, contexte du fichier entier sous forme de chaîne)\n",
    "\n",
    "\n",
    "Le code suivant démontre cela sur un exemple du nombre de mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Prendre les 10 mots les plus fréquents dans le texte et les fréquences correspondantes\n",
      "[('the', 22635), ('of', 11167), ('and', 11086), ('to', 10707), ('a', 10433), ('I', 10183), ('in', 7006), ('that', 6911), ('was', 6779), ('his', 4955)]\n",
      "Elapsed time:  10.182549715042114\n"
     ]
    }
   ],
   "source": [
    "#from pyspark import SparkContext\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "inputfile=\"file:///home/mbds/spark/tpSpark/2_LoadingData/data/unstructured/\"\n",
    "outputdir=\"file:///home/mbds/spark/tpSpark/2_LoadingData/output/\"\n",
    "start = time.time()\n",
    "input_rdd = sc.textFile(inputfile)\n",
    "counts = input_rdd.flatMap(lambda line: line.split()). \\\n",
    "                                                    map(lambda word: (word, 1)). \\\n",
    "                                                    reduceByKey(lambda a, b: a + b)\n",
    "print (\"\\n Prendre les 10 mots les plus fréquents dans le texte et les fréquences correspondantes\")\n",
    "print (counts.takeOrdered(10, key=lambda x: -x[1]))\n",
    "counts.map(lambda x: (x[1],x[0])).sortByKey(0).map(lambda x: (x[1],x[0]))\\\n",
    "        .repartition(1).saveAsTextFile(outputdir+\"output_loadunstructured1/\")\n",
    "end = time.time()\n",
    "print (\"Elapsed time: \", (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.wholeTextFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['file:/home/mbds/spark/tpSpark/2_LoadingData/data/unstructured/retn.txt'],\n",
       " ['file:/home/mbds/spark/tpSpark/2_LoadingData/data/unstructured/case.txt'],\n",
       " ['file:/home/mbds/spark/tpSpark/2_LoadingData/data/unstructured/advs.txt'],\n",
       " ['file:/home/mbds/spark/tpSpark/2_LoadingData/data/unstructured/mems.txt'],\n",
       " ['file:/home/mbds/spark/tpSpark/2_LoadingData/data/unstructured/lstb.txt']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pair_rdd = sc.wholeTextFiles(inputfile)\n",
    "counts = input_pair_rdd.map(lambda line: line[0].split())\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_pair_rdd = sc.wholeTextFiles(inputfile)\n",
    "counts = input_pair_rdd.map(lambda line: line[1].split())\n",
    "counts.take(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Prendre les 10 mots les plus fréquents dans le texte et les fréquences correspondantes\n",
      "[('the', 22635), ('of', 11167), ('and', 11086), ('to', 10707), ('a', 10433), ('I', 10183), ('in', 7006), ('that', 6911), ('was', 6779), ('his', 4955)]\n",
      "Elapsed time:  2.1484148502349854\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "input_rdd = sc.wholeTextFiles(inputfile)\n",
    "counts = input_rdd.flatMap(lambda line: line[1].split()). \\\n",
    "                                                    map(lambda word: (word, 1)). \\\n",
    "                                                    reduceByKey(lambda a, b: a + b)\n",
    "print (\"\\n Prendre les 10 mots les plus fréquents dans le texte et les fréquences correspondantes\")\n",
    "print (counts.takeOrdered(10, key=lambda x: -x[1]))\n",
    "counts.map(lambda x: (x[1],x[0])).sortByKey(0).map(lambda x: (x[1],x[0]))\\\n",
    "        .repartition(1).saveAsTextFile(outputdir+\"output_loadunstructured1/\")\n",
    "end = time.time()\n",
    "print (\"Elapsed time: \", (end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du CSV\n",
    "\n",
    "Ensuite, nous allons apprendre à charger des données dans un format structuré comme CSV. Il existe au moins deux façons de procéder:\n",
    "\n",
    "1. Lisez les fichiers ligne par ligne avec la méthode `textFiles ()`, divisée en délimiteur (non recommandé). Cela produira un RDD qui est une structure de données optimisée pour l'analyse orientée lignes et des primitives fonctionnelles telles que `map` et` filter`\n",
    "1. Lisez les fichiers CSV en utilisant le `DataFrameReader` intégré (recommandé). Cela produira un dataframe, qui est une structure de données optimisée pour l'analyse orientée colonnes et les primitives relationnelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import os\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO\n",
    "\n",
    "#this one is use when you use textFile\n",
    "def loadRecord(line,header,delimiter):\n",
    "    \"\"\"Parse a CSV line\"\"\"\n",
    "    input = StringIO(line)\n",
    "    reader = csv.DictReader(input, delimiter=delimiter, fieldnames=header)\n",
    "    return next(reader)\n",
    "\n",
    "delimiter = \"|\"\n",
    "inputfile=\"file:///home/mbds/spark/tpSpark/2_LoadingData\"\n",
    "outputdir=\"file:///home/mbds/spark/tpSpark/2_LoadingData/output\"\n",
    "\n",
    "input = sc.textFile(inputfile+\"/data/csv/person_nodes.csv\")\n",
    "header = input.first().split(delimiter)\n",
    "data = input.filter(lambda x: header[0] not in x).map(lambda x: loadRecord(x,header,delimiter))\n",
    "data.repartition(1).saveAsTextFile(outputdir+\"/output_csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = spark.read.options(header='true', inferschema='true',delimiter=delimiter).csv(inputfile+\"/data/csv/person_nodes.csv\")\n",
    "input_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(outputdir+\"/output_csv2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple: analyse DB de diamants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la même manière qu'avant, nous allons lire le fichier CSV dans dataframe.\n",
    "Spark DataFrameReader peut gérer les délimiteurs et peut éventuellement ignorer la ligne d'en-tête pour les fichiers CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv data as DataFrame using spark csv dataframe reader\n",
    "diamonds = spark.read.options(header='true', inferSchema='true').csv(inputfile+'/data/csv/diamonds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|_c0|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|  1| 0.23|    Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "|  2| 0.21|  Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "|  3| 0.23|     Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "|  4| 0.29|  Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n",
      "|  5| 0.31|     Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n",
      "|  6| 0.24|Very Good|    J|   VVS2| 62.8| 57.0|  336|3.94|3.96|2.48|\n",
      "|  7| 0.24|Very Good|    I|   VVS1| 62.3| 57.0|  336|3.95|3.98|2.47|\n",
      "|  8| 0.26|Very Good|    H|    SI1| 61.9| 55.0|  337|4.07|4.11|2.53|\n",
      "|  9| 0.22|     Fair|    E|    VS2| 65.1| 61.0|  337|3.87|3.78|2.49|\n",
      "| 10| 0.23|Very Good|    H|    VS1| 59.4| 61.0|  338| 4.0|4.05|2.39|\n",
      "+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diamonds.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- carat: double (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: double (nullable = true)\n",
      " |-- table: double (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diamonds.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recuperer combine de ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53940"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|color|\n",
      "+-----+\n",
      "|    F|\n",
      "|    E|\n",
      "|    D|\n",
      "|    J|\n",
      "|    G|\n",
      "|    I|\n",
      "|    H|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diamonds.select('color').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, essayons d'estimer un prix moyen par carat. Comme vous l'avez remarqué, la colonne de prix est un entier. Cela peut entraîner une perte de précision lors du calcul de la moyenne. Donc, tout d'abord, nous allons convertir cette colonne en type double:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- carat: double (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: double (nullable = true)\n",
      " |-- table: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Convert Price column to type DoubleType\n",
    "diamondsdf = diamonds.withColumn(\"price\", diamonds[\"price\"].cast(DoubleType()))\n",
    "diamondsdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utiliserons la \"fonction groupby-aggregate\" pour calculer la moyenne. Cela crée une colonne avec le nom par défaut \"avg (prix)\" que nous renommons en quelque chose de plus facile à taper. Enfin, nous classons la sortie par prix dans l'ordre décroissant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|carat|          avgPrice|\n",
      "+-----+------------------+\n",
      "| 3.51|           18701.0|\n",
      "| 2.67|           18686.0|\n",
      "|  4.5|           18531.0|\n",
      "| 5.01|           18018.0|\n",
      "| 2.57|17841.666666666668|\n",
      "|  2.6|           17535.0|\n",
      "| 2.64|           17407.0|\n",
      "| 4.13|           17329.0|\n",
      "| 2.39|17182.428571428572|\n",
      "| 2.71|           17146.0|\n",
      "+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate average price per carat value\n",
    "carat_avgPrice = (diamondsdf\n",
    "                  .groupBy(\"carat\")\n",
    "                  .avg(\"price\")\n",
    "                  .withColumnRenamed(\"avg(price)\", \"avgPrice\")\n",
    "                  .orderBy(desc(\"avgPrice\")))\n",
    "\n",
    "# View top10 highest carat value\n",
    "carat_avgPrice.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (approche non recommandée) Analyse des fichiers CSV en Python en tant que RDD \n",
    "\n",
    "En principe, on peut également utiliser `RDD` pour analyser des données structurées, mais cela semble moins utiles, surtout si la logique de votre analyse peut être exprimée à l'aide des fonctions relationnelles de  SQL.\n",
    "\n",
    "Nous allons maintenant convertir nos diamants DataFrame en RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can convert the DataFrame directly into an RDD\n",
    "diamonds_rdd = diamonds.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=1, carat=0.23, cut='Ideal', color='E', clarity='SI2', depth=61.5, table=55.0, price=326, x=3.95, y=3.98, z=2.43),\n",
       " Row(_c0=2, carat=0.21, cut='Premium', color='E', clarity='SI1', depth=59.8, table=61.0, price=326, x=3.89, y=3.84, z=2.31),\n",
       " Row(_c0=3, carat=0.23, cut='Good', color='E', clarity='VS1', depth=56.9, table=65.0, price=327, x=4.05, y=4.07, z=2.31)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 3 rows of the diamonds RDD\n",
    "diamonds_rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant utiliser les opérations RDD pour analyser les données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ideal', 21551), ('Premium', 13791), ('Good', 4906)]\n"
     ]
    }
   ],
   "source": [
    "# Diamond counts by cuts\n",
    "countByGroup = diamonds_rdd.map(lambda x: (x.cut, 1)).reduceByKey(lambda x,y: x+y)\n",
    "print (countByGroup.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SI2', 'SI1', 'VS1', 'VS2', 'VVS2', 'VVS1', 'I1', 'IF']\n"
     ]
    }
   ],
   "source": [
    "# Distinct diamond clarities in dataset\n",
    "distinctClarity = diamonds_rdd.map(lambda x: x.clarity).distinct()\n",
    "print (distinctClarity.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ideal', 2756.7240663718817), ('Premium', 2756.654813661215), ('Good', 2755.647409027791), ('Very Good', 2756.7183661747795), ('Fair', 2743.567771968392)]\n"
     ]
    }
   ],
   "source": [
    "# Average price per diamond cut\n",
    "avgPrice = diamonds_rdd.map(lambda x: (x.cut, float(x.price))).reduceByKey(lambda x,y: (x+y)/2)\n",
    "print (avgPrice.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice: charger un fichier CSV et l'analyser\n",
    "\n",
    "Utilisez ce que vous avez appris pour charger un ensemble de jeux de données `CSV`. Ouvrez ** load_csv_exercise.ipynb ** et suivez le devoir qui y figure.\n",
    "\n",
    "1. Acteur\n",
    "1. Film\n",
    "1. Acteur jouant dans un film (relations)\n",
    "\n",
    "et trouvez des films dans lesquels ** Tom Hanks ** a joué.\n",
    "\n",
    "Enregistrez la réponse au format «JSON»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement de JSON\n",
    "\n",
    "Le meilleur et le seul moyen raisonnable de charger des fichiers JSON consiste à utiliser Spark DataFrameReader.\n",
    "Spark SQL prend en charge la lecture de fichiers JSON contenant un objet JSON distinct et autonome par ligne.\n",
    "\n",
    "** Remarque: les fichiers JSON multilignes ne sont actuellement pas compatibles avec Spark SQL. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "testJsonData = spark.read.json(inputfile+\"/data/json/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- array: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- dict: struct (nullable = true)\n",
      " |    |-- extra_key: string (nullable = true)\n",
      " |    |-- key: string (nullable = true)\n",
      " |-- int: long (nullable = true)\n",
      " |-- string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-------+\n",
      "|    array|                dict|int| string|\n",
      "+---------+--------------------+---+-------+\n",
      "|[1, 2, 3]|          [, value1]|  1|string1|\n",
      "|[2, 4, 6]|          [, value2]|  2|string2|\n",
      "|[3, 6, 9]|[extra_value3, va...|  3|string3|\n",
      "+---------+--------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL peut déduire le schéma automatiquement à partir de vos données JSON. Pour afficher le schéma, utilisez `printSchema`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons maintenant de faire quelques requêtes de base pour mieux comprendre l'ensemble de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Count number of rows in dataset\n",
    "print (testJsonData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données JSON peuvent contenir des structures de données imbriquées auxquelles on peut accéder par \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   key|\n",
      "+------+\n",
      "|value1|\n",
      "|value2|\n",
      "|value3|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonData.select('dict.key').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform DataFrame operations such as filtering queries according to some criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-------+\n",
      "|    array|                dict|int| string|\n",
      "+---------+--------------------+---+-------+\n",
      "|[2, 4, 6]|       [null,value2]|  2|string2|\n",
      "|[3, 6, 9]|[extra_value3,val...|  3|string3|\n",
      "+---------+--------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testJsonData.filter(testJsonData[\"int\"] > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des fichiers JSON en Python avec SQL\n",
    "Tout DataFrame, y compris ceux créés avec des données JSON, peut être inscrit en tant que table Spark SQL pour interroger avec SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark SQL temp table\n",
    "# Note that temp tables are not global across clusters and will not persist across cluster restarts\n",
    "testJsonData.registerTempTable(\"test_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons exécuter toutes les requêtes SQL sur cette table avec Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---+-------+\n",
      "|    array|                dict|int| string|\n",
      "+---------+--------------------+---+-------+\n",
      "|[1, 2, 3]|          [, value1]|  1|string1|\n",
      "|[2, 4, 6]|          [, value2]|  2|string2|\n",
      "|[3, 6, 9]|[extra_value3, va...|  3|string3|\n",
      "+---------+--------------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM test_json\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-exercise\n",
    "\n",
    "Basculez vers le répertoire de travail du TP, ouvrez le fichier: ** load_json.ipynb **\n",
    "et suivez les instructions en ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = spark.read.json(inputfile+\"/data/json/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
